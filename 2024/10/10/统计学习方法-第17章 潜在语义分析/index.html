<!DOCTYPE html><html class="hide-aside" lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>统计学习方法--第17章 潜在语义分析 | LiuJT's Blog</title><meta name="author" content="LiuJT"><meta name="copyright" content="LiuJT"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="潜在语义分析（latent semantic analysis, LSA）是一种无监督学习方法，主要用于文本的话题分析，其特点是通过矩阵分解发现文本与单词之间的基于话题的语义关系。文本信息处理中，  传统方法：以单词向量表示文本的语义内容，以单词向量空间的度量表示文本之间的语义相似度。 潜在语义分析：旨在解决传统方法不能准确表示语义的问题，试图从大量的文本数据中发现潜在的话题，以话题向量表示文本的">
<meta property="og:type" content="article">
<meta property="og:title" content="统计学习方法--第17章 潜在语义分析">
<meta property="og:url" content="http://example.com/2024/10/10/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC17%E7%AB%A0%20%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="LiuJT&#39;s Blog">
<meta property="og:description" content="潜在语义分析（latent semantic analysis, LSA）是一种无监督学习方法，主要用于文本的话题分析，其特点是通过矩阵分解发现文本与单词之间的基于话题的语义关系。文本信息处理中，  传统方法：以单词向量表示文本的语义内容，以单词向量空间的度量表示文本之间的语义相似度。 潜在语义分析：旨在解决传统方法不能准确表示语义的问题，试图从大量的文本数据中发现潜在的话题，以话题向量表示文本的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2024/08/28/1HRJu725plD8QAw.jpg">
<meta property="article:published_time" content="2024-10-09T16:00:00.000Z">
<meta property="article:modified_time" content="2024-10-15T07:51:22.226Z">
<meta property="article:author" content="LiuJT">
<meta property="article:tag" content="统计学习方法">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2024/08/28/1HRJu725plD8QAw.jpg"><link rel="shortcut icon" href="https://s2.loli.net/2024/08/28/WOMi84ksFx3dGY1.jpg"><link rel="canonical" href="http://example.com/2024/10/10/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC17%E7%AB%A0%20%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '统计学习方法--第17章 潜在语义分析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-15 15:51:22'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/28/WOMi84ksFx3dGY1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 索引</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-calendar"></i><span> 时间</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-cogs"></i><span> 分类</span></a></li><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-archive"></i><span> 统计学习方法</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-archive"></i><span> 概率理论</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 常用软件</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Pycharm/"><i class="fa-fw fas fa-cogs"></i><span> Pycharm</span></a></li><li><a class="site-page child" href="/LaTeX/"><i class="fa-fw fas fa-cogs"></i><span> LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url('https://s2.loli.net/2024/08/28/1HRJu725plD8QAw.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="LiuJT's Blog"></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 索引</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-calendar"></i><span> 时间</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-cogs"></i><span> 分类</span></a></li><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-archive"></i><span> 统计学习方法</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-archive"></i><span> 概率理论</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 常用软件</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Pycharm/"><i class="fa-fw fas fa-cogs"></i><span> Pycharm</span></a></li><li><a class="site-page child" href="/LaTeX/"><i class="fa-fw fas fa-cogs"></i><span> LaTeX</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">统计学习方法--第17章 潜在语义分析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2024-10-09T16:00:00.000Z" title="发表于 2024-10-10 00:00:00">2024-10-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="统计学习方法--第17章 潜在语义分析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>潜在语义分析（latent semantic analysis, LSA）是一种无监督学习方法，主要用于<strong>文本的话题分析</strong>，其特点是<strong>通过矩阵分解</strong>发现文本与单词之间的基于话题的语义关系。文本信息处理中，</p>
<ul>
<li>传统方法：以<strong>单词向量</strong>表示文本的语义内容，以<strong>单词向量空间的度量表示文本之间的语义相似度</strong>。</li>
<li>潜在语义分析：旨在解决<strong>传统方法不能准确表示语义的问题</strong>，试图从大量的文本数据中发现潜在的话题，以<strong>话题向量</strong>表示文本的语义内容，以<strong>话题向量空间的度量更准确地表示文本之间的语义相似度</strong>。这也是话题分析（topic modeling）的基本想法。<ul>
<li>定性：潜在语义分析使用的是<strong>非概率</strong>的话题分析模型。</li>
<li>具体做法：首先将文本集合表示为单词-文本矩阵，对单词-文本矩阵进行奇异值分解，从而得到话题向量空间，以及文本在话题向量空间的表示。</li>
<li>另外，非负矩阵分解也可以用于话题分析。非负矩阵分解（non-negative matrix factorization，NMF）是另一种矩阵的因子分解方法，其特点是分解的矩阵非负。</li>
</ul>
</li>
</ul>
<h3 id="1-单词、话题向量空间"><a href="#1-单词、话题向量空间" class="headerlink" title="1. 单词、话题向量空间"></a>1. 单词、话题向量空间</h3><h4 id="1-1-单词向量空间"><a href="#1-1-单词向量空间" class="headerlink" title="1.1 单词向量空间"></a>1.1 单词向量空间</h4><ul>
<li>文本信息处理，比如文本信息检索、文本数据挖掘的一个核心问题：对文本的语义内容进行表示, 并进行文本之间的语义相似度计算。</li>
<li>最简单的方法是利用向量空间模型 (vector space model, VSM), 也就是单词向量空间模型 （ word vector space model）：<ul>
<li><strong>基本想法</strong>： 给定一个文本, 用一个向量表示该文本的 “语义”, 向量的每一维对应一个单词, 其数值为该单词在该文本中出现的频数或权值;</li>
<li><strong>基本假设</strong>：<ul>
<li>文本中所有单词的出现情况表示了文本的语义内容；</li>
<li>文本集合中的每个文本都表示为一个向量, 存在于一个向量空间;</li>
<li>向量空间的度量, 如内积或标准化内积表示文本之间的”语义相似度”。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>定义 1</strong>：给定一个含有 $n$ 个文本的集合 $D=\left{d<em>{1}, d</em>{2}, \cdots, d<em>{n}\right}$, 以及在所有文本中出现的 $m$ 个单词的集合 $W=\left{w</em>{1}, w<em>{2}, \cdots, w</em>{m}\right}$ 。将单词在文本中出现的数据用一个单词-文本矩阵（word-document matrix）表示, 记作 $X$</p>
<script type="math/tex; mode=display">
X=\left[\begin{array}{llll}x_{1} & x_{2} & \cdots & x_{n}\end{array}\right]=
\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 n} \\
x_{21} & x_{22} & \cdots & x_{2 n} \\
\vdots & \vdots & & \vdots \\
x_{m 1} & x_{m 2} & \cdots & x_{m n}
\end{array}\right]</script><p>这是一个 $m \times n$ 矩阵, 元素 $x<em>{i j}$ 表示单词 $w</em>{i}$ 在文本 $d_{j}$ 中出现的频数或权值。<strong>由于单词的种类很多, 而每个文本中出现单词的种类通常较少, 所以单词-文本矩阵是一个稀疏矩阵</strong>。权值通常用单词频率-逆文本频率 （term frequency-inverse document frequency, TF-IDF）表示，定义</p>
<script type="math/tex; mode=display">
\begin{equation*}
\mathrm{TFIDF}_{i j}=\frac{\mathrm{tf}_{i j}}{\mathrm{tf}_{\bullet j}} \log \frac{\mathrm{df}}{\mathrm{df}_{i}}, \quad i=1,2, \cdots, m ; \quad j=1,2, \cdots, n
\end{equation*}</script><ul>
<li>$\mathrm{tf}<em>{i j}$ 是单词 $w</em>{i}$ 出现在文本 $d_{j}$ 中的频数</li>
<li>$\mathrm{tf}<em>{\bullet}{ }</em>{j}$ 是文本 $d_{j}$ 中出现的所有单词的频数之和</li>
<li>$\mathrm{df}<em>{i}$ 是含有单词 $w</em>{i}$ 的文本数,</li>
<li>df 是文本集合 $D$ 的全部文本数。</li>
</ul>
<p>直观上,</p>
<ul>
<li>一个单词在<strong>一个文本中出现的频数越高</strong>, 这个单词在这个文本中的<strong>重要度就越高</strong>;</li>
<li>一个单词在<strong>整个文本集合中出现的文本数越少</strong>, 这个单词就越能表示其所在文本的特点, <strong>重要度就越高</strong>;</li>
<li>一个单词在一个文本的 TF-IDF 是两种重要度的积, 表示综合重要度。</li>
</ul>
<p><strong>单词向量空间模型直接使用单词-文本矩阵的信息</strong>。单词-文本矩阵的第 $j$ 列向量 $x<em>{j}$ 表示文本 $d</em>{j}$，<strong>两个单词向量的内积或标准化内积（余弦）表示对应的文本之间的语义相似度</strong>。文本 $d<em>{i}$ 与 $d</em>{j}$ 之间的相似度为</p>
<script type="math/tex; mode=display">
\begin{equation*}
x_{i} \cdot x_{j}, \quad \frac{x_{i} \cdot x_{j}}{\left\|x_{i}\right\|\left\|x_{j}\right\|}
\end{equation*}</script><p>直观上，在两个文本中共同出现的单词越多，其语义内容就越相近，这时，对应的单词向量同不为零的维度就越多，内积就越大（单词向量元素的值都是非负的），表示两个文本在语义内容上越相似。</p>
<p><strong>单词向量空间模型优点</strong>：模型简单，计算效率高。因为单词向量通常是稀疏的，两个向量的内积计算只需要在其同不为零的维度上进行即可，需要的计算很少，可以高效地完成。<br><strong>单词向量空间模型缺点</strong>：内积相似不等于文本的语义相似度。因为同一个单词可以表示多个语义，多个单词可以表示同一个语义，所以基于单词向量的相似度计算存在不精确的问题。</p>
<h4 id="1-2-话题向量空间"><a href="#1-2-话题向量空间" class="headerlink" title="1.2 话题向量空间"></a>1.2 话题向量空间</h4><p><strong>存在问题</strong>：为了解决单词向量空间模型的问题，提出基于话题的向量模型。<br><strong>解决方法</strong>：因为两个文本的语义相似度可以体现在两者的话题相似度上。如果两个文本的话题相似，那么两者的语义应该也相似。话题可以由若干个语义相关的单词表示，同义词可以表示同一个话题，而多义词可以表示不同的话题。</p>
<p>话题向量空间模型（topic vector space model）：给定一个文本，用话题空间的一个向量表示该文本，该向量的每一分量对应一个话题，其数值为该话题在该文本中出现的权值。用两个向量的内积或标准化内积表示对应的两个文本的语义相似度。</p>
<p><strong>定义 2</strong>：假设所有文本共含有 $k$ 个话题。假设每个话题由一个定义在单词集合 $W$ 上的 $m$ 维向量表示, 称为话题向量, 即</p>
<script type="math/tex; mode=display">
t_{l}=\left[\begin{array}{c}
t_{1 l}  \\
t_{2 l} \\
\vdots \\
t_{m l}
\end{array}\right], \quad l=1,2, \cdots, k</script><p>其中 $t<em>{i l}$ 是单词 $w</em>{i}$ 在话题 $t<em>{l}$ 的权值， $i=1,2, \cdots, m$ ，权值越大，该单词在该话题中的重要度就越高。这 $k$ 个话题向量 $t</em>{1}, t<em>{2}, \cdots, t</em>{k}$ 张成一个话题向量空间 （topic vector space），维数为 $k$ 。注意话题向量空间 $T$ 是单词向量空间 $X$ 的一个子空间。</p>
<p>话题向量空间 $T$ 也可以表示为一个矩阵, 称为单词-话题矩阵（word-topic matrix），记作</p>
<script type="math/tex; mode=display">
T=\left[\begin{array}{llll}t_{1} & t_{2} & \cdots & t_{k}\end{array}\right]=\left[\begin{array}{cccc}
t_{11} & t_{12} & \cdots & t_{1 k} \\
t_{21} & t_{22} & \cdots & t_{2 k} \\
\vdots & \vdots & & \vdots \\
t_{m 1} & t_{m 2} & \cdots & t_{m k}
\end{array}\right]</script><p>现在考虑文本集合 $D$ 的文本 $d<em>{j}$ ，在单词向量空间中由一个向量 $x</em>{j}$ 表示，将 $x<em>{j}$投影到话题向量空间 $T$ 中，得到在话题向量空间的一个向量 $y</em>{j}, y_{j}$ 是一个 $k$ 维向量，其表达式为</p>
<script type="math/tex; mode=display">
y_{j}=\left[\begin{array}{c}
y_{1 j} \\
y_{2 j} \\
\vdots \\
y_{k j}
\end{array}\right], \quad j=1,2, \cdots, n</script><p>其中 $y<em>{l j}$ 是文本 $d</em>{j}$ 在话题 $t_{l}$ 的权值, $l=1,2, \cdots, k$, 权值越大, 该话题在该文本中的重要度就越高。</p>
<p>矩阵 $Y$ 表示话题在文本中出现的情况，称为话题-文本矩阵（topic-document<br>matrix），记作</p>
<script type="math/tex; mode=display">
Y=\left[\begin{array}{llll}y_{1} & y_{2} & \cdots & y_{n}\end{array}\right]=\left[\begin{array}{cccc}
y_{11} & y_{12} & \cdots & y_{1 n} \\
y_{21} & y_{22} & \cdots & y_{2 n} \\
\vdots & \vdots & & \vdots \\
y_{k 1} & y_{k 2} & \cdots & y_{k n}
\end{array}\right]</script><p><strong>如何从单词向量空间到话题向量空间的线性变换？</strong></p>
<p>单词向量空间的文本向量 $x<em>{j}$ 可以通过它在话题空间中的向量 $y</em>{j}$ 线性组合近似表示</p>
<script type="math/tex; mode=display">
\begin{equation*}
x_{j} \approx y_{1 j} t_{1}+y_{2 j} t_{2}+\cdots+y_{k j} t_{k}, \quad j=1,2, \cdots, n
\end{equation*}</script><p>所以，单词-文本矩阵 $X$ 可以近似的表示为单词-话题矩阵 $T$ 与话题-文本矩阵 $Y$ 的乘积形式</p>
<script type="math/tex; mode=display">
\begin{equation*}
X \approx T Y
\end{equation*}</script><p>在原始的单词向量空间中，两个文本 $d<em>{i}$ 与 $d</em>{j}$ 的相似度可以由对应的向量的内积表示，即 $x<em>{i} \cdot x</em>{j}$ 。经过潜在语义分析之后，在话题向量空间中，两个文本 $d<em>{i}$ 与 $d</em>{j}$ 的相似度可以由对应的向量的内积即 $y<em>{i} \cdot y</em>{j}$ 表示。</p>
<h3 id="2-潜在语义分析算法"><a href="#2-潜在语义分析算法" class="headerlink" title="2. 潜在语义分析算法"></a>2. 潜在语义分析算法</h3><p>潜在语义分解利用奇异值分解方法，对单词文本矩阵做奇异值分解：</p>
<ul>
<li>左矩阵作为话题向量空间</li>
<li>对角矩阵与右矩阵的乘积作为文本在话题向量空间的表示</li>
</ul>
<script type="math/tex; mode=display">
\begin{align*}
X & =\left[\begin{array}{llll}
x_{1} & x_{2} & \cdots & x_{n}
\end{array}\right] \approx U_{k} \Sigma_{k} V_{k}^{\mathrm{T}} \\
& =\left[\begin{array}{llll}
u_{1} & u_{2} & \cdots & u_{k}
\end{array}\right]\left[\begin{array}{cccc}
\sigma_{1} & & & \\
& \sigma_{2} & 0 & \\
0 & \ddots & \\
& & \sigma_{k}
\end{array}\right]\left[\begin{array}{cccc}
v_{11} & v_{21} & \cdots & v_{n 1} \\
v_{12} & v_{22} & \cdots & v_{n 2} \\
\vdots & \vdots & & \vdots \\
v_{1 k} & v_{2 k} & \cdots & v_{n k}
\end{array}\right] \\
& =\left[\begin{array}{llll}
u_{1} & u_{2} & \cdots & u_{k}
\end{array}\right]\left[\begin{array}{cccc}
\sigma_{1} v_{11} & \sigma_{1} v_{21} & \cdots & \sigma_{1} v_{n 1} \\
\sigma_{2} v_{12} & \sigma_{2} v_{22} & \cdots & \sigma_{2} v_{n 2} \\
\vdots & \vdots & & \vdots \\
\sigma_{k} v_{1 k} & \sigma_{k} v_{2 k} & \cdots & \sigma_{k} v_{n k}
\end{array}\right]
\end{align*}</script><p>其中</p>
<script type="math/tex; mode=display">
u_{l}=\left[\begin{array}{c}
u_{1 l} \\
u_{2 l} \\
\vdots \\
u_{m l}
\end{array}\right], \quad l=1,2, \cdots, k</script><p>于是可以通过对单词-文本矩阵的奇异值分解进行潜在语义分析</p>
<script type="math/tex; mode=display">
\begin{equation*}
X \approx U_{k} \Sigma_{k} V_{k}^{\mathrm{T}}=U_{k}\left(\Sigma_{k} V_{k}^{\mathrm{T}}\right)
\end{equation*}</script><p>得到话题空间 $U<em>{k}$, 以及文本在话题空间的表示 $\left(\Sigma</em>{k} V_{k}^{\mathrm{T}}\right)$ 。</p>
<h3 id="3-非负矩阵分解算法"><a href="#3-非负矩阵分解算法" class="headerlink" title="3. 非负矩阵分解算法"></a>3. 非负矩阵分解算法</h3><p>非负矩阵分解也可以用于话题分析：</p>
<ul>
<li>左矩阵作为话题向量空间</li>
<li>右矩阵作为文本在话题向量空间的表示</li>
</ul>
<h4 id="3-1-非负矩阵分解"><a href="#3-1-非负矩阵分解" class="headerlink" title="3.1 非负矩阵分解"></a>3.1 非负矩阵分解</h4><p><strong>非负矩阵</strong>：若一个矩阵的所有元素非负，则称该矩阵为非负矩阵，记作 $X \geqslant 0$ 。</p>
<p><strong>非负矩阵分解</strong>：给定一个非负矩阵 $X \geqslant 0$, 找到两个非负矩阵 $W \geqslant 0$ 和 $H \geqslant 0$, 使得</p>
<script type="math/tex; mode=display">
\begin{equation*}
X \approx W H
\end{equation*}</script><p>因为 $W H$ 与 $X$ 完全相等很难实现，所以只要求 $W H$ 与 $X$ 近似相等。可以看到非负矩阵分解也是对数据进行压缩。</p>
<h4 id="3-2-潜在语义分析模型"><a href="#3-2-潜在语义分析模型" class="headerlink" title="3.2 潜在语义分析模型"></a>3.2 潜在语义分析模型</h4><p>给定一个 $m \times n$ 非负的单词-文本矩阵 $X \geqslant 0$ 。假设文本集合共包含 $k$ 个话题，对 $X$ 进行非负矩阵分解。即求非负的 $m \times k$ 矩阵 $W \geqslant 0$ 和 $k \times n$ 矩阵 $H \geqslant 0$ ，使得</p>
<script type="math/tex; mode=display">
\begin{equation*}
X \approx W H
\end{equation*}</script><ul>
<li>$W=\left[\begin{array}{llll}w<em>{1} &amp; w</em>{2} &amp; \cdots &amp; w<em>{k}\end{array}\right]$ 为话题向量空间， $w</em>{1}, w<em>{2}, \cdots, w</em>{k}$ 表示文本集合的 $k$ 个话题</li>
<li>$H=\left[\begin{array}{llll}h<em>{1} &amp; h</em>{2} &amp; \cdots &amp; h<em>{n}\end{array}\right]$ 为文本在话题向量空间的表示， $h</em>{1}, h<em>{2}, \cdots, h</em>{n}$ 表示文本集合的 $n$ 个文本。</li>
</ul>
<h4 id="3-3-非负矩阵分解的形式化"><a href="#3-3-非负矩阵分解的形式化" class="headerlink" title="3.3 非负矩阵分解的形式化"></a>3.3 非负矩阵分解的形式化</h4><p>非负矩阵分解可以形式化为最优化问题求解。</p>
<p>首先定义损失函数：设两个非负矩阵 $A=\left[a<em>{i j}\right]</em>{m \times n}$ 和 $B=\left[b<em>{i j}\right]</em>{m \times n}$</p>
<ul>
<li><p>平方损失</p>
<script type="math/tex; mode=display">
\begin{equation*}
\|A-B\|^{2}=\sum_{i, j}\left(a_{i j}-b_{i j}\right)^{2}
\end{equation*}</script><p>其下界是 0 ，当且仅当 $A=B$ 时达到下界。</p>
</li>
<li><p>散度（divergence）</p>
<script type="math/tex; mode=display">
\begin{equation*}
D(A \| B)=\sum_{i, j}\left(a_{i j} \log \frac{a_{i j}}{b_{i j}}-a_{i j}+b_{i j}\right)
\end{equation*}</script><p>其下界也是 0 ，当且仅当 $A=B$ 时达到下界。 $A$ 和 $B$ 不对称。当 $\sum<em>{i, j} a</em>{i j}=\sum<em>{i, j} b</em>{i j}=1$ 时散度损失函数退化为 Kullback-Leiber 散度或相对熵, 这时 $A$ 和 $B$ 是概率分布。</p>
</li>
</ul>
<p>接着定义以下的最优化问题：</p>
<ul>
<li><p>目标函数 $|X-W H|^{2}$ 关于 $W$ 和 $H$ 的最小化，满足约束条件 $W, H     \geqslant 0$ ，即</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\min _{W, H} & \|X-W H\|^{2}  \\
\text { s.t. } & W, H \geqslant 0
\end{array}</script></li>
<li><p>目标函数 $D(X | W H)$ 关于 $W$ 和 $H$ 的最小化, 满足约束条件 $W, H \geqslant 0$, 即</p>
<script type="math/tex; mode=display">
\begin{equation*}
\min _{W, H} D(X \| W H)
\end{equation*}</script><p>s.t. $W, H \geqslant 0$</p>
</li>
</ul>
<h4 id="3-4-算法"><a href="#3-4-算法" class="headerlink" title="3.4 算法"></a>3.4 算法</h4><p>由于目标函数 $|X-W H|^{2}$ 和 $D(X | W H)$ 只是对变量 $W$ 和 $H$ 之一的凸函数，而不是同时对两个变量的凸函数，因此找到全局最优（最小值）比较困难，可以通过数值最优化方法求局部最优（极小值）。</p>
<ul>
<li>梯度下降法比较容易实现，但是收玫速度慢。</li>
<li>共轭梯度法收玫速度快，但实现比较复杂。</li>
<li>Lee 和 Seung 提出了新的基于 “乘法更新规则” 的优化算法, 交替地对 $W$ 和 $H$进行更新，其理论依据是下面的定理。</li>
</ul>
<p>乘法更新规则：</p>
<ul>
<li><p>平方损失 $|X-W H|^{2}$ 对下列乘法更新规则</p>
<script type="math/tex; mode=display">
\begin{align*}
& H_{l j} \leftarrow H_{l j} \frac{\left(W^{\mathrm{T}} X\right)_{l j}}{\left(W^{\mathrm{T}} W H\right)_{l j}}  \\
& W_{i l} \leftarrow W_{i l} \frac{\left(X H^{\mathrm{T}}\right)_{i l}}{\left(W H H^{\mathrm{T}}\right)_{i l}}
\end{align*}</script><p>是非增的。当且仅当 $W$ 和 $H$ 是平方损失函数的稳定点时函数的更新不变。</p>
</li>
<li><p>散度损失 $D(X-W H)$ 对下列乘法更新规则</p>
<script type="math/tex; mode=display">
\begin{array}{r}
H_{l j} \leftarrow H_{l j} \frac{\sum_{i}\left[W_{i l} X_{i j} /(W H)_{i j}\right]}{\sum_{i} W_{i l}} \\
W_{i l} \leftarrow W_{i l} \frac{\sum_{j}\left[H_{l j} X_{i j} /(W H)_{i j}\right]}{\sum_{j} H_{l j}}
\end{array}</script><p>是非增的。当且仅当 $W$ 和 $H$ 是散度损失函数的稳定点时函数的更新不变。</p>
</li>
</ul>
<p><strong>非负矩阵分解的迭代算法</strong><br>输入：单词-文本矩阵 $X \geqslant 0$ ，文本集合的话题个数 $k$ ，最大迭代次数 $t$;<br>输出：话题矩阵 $W$ ，文本表示矩阵 $H$ 。<br>（1）初始化：$W \geqslant 0$ ，并对 $W$ 的每一列数据归一化；$H \geqslant 0 ;$<br>（2）迭代：对迭代次数由 1 到 $t$ 执行下列步骤:</p>
<ul>
<li>(a) 更新 $W$ 的元素，对 $l$ 从 1 到 $k, i$ 从 1 到 $m$ 更新 $W_{i l}$ ；</li>
<li>(b) 更新 $H$ 的元素, 对 $l$ 从 1 到 $k, j$ 从 1 到 $n$ 更新 $H_{l j}$ 。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">LiuJT</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/10/10/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC17%E7%AB%A0%20%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/">http://example.com/2024/10/10/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC17%E7%AB%A0%20%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">LiuJT's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">统计学习方法</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2024/08/28/1HRJu725plD8QAw.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/10/10/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC20%E7%AB%A0%20LDA/" title="统计学习方法--第20章 LDA 模型"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/28/uDX9laHIOJK6oMv.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">统计学习方法--第20章 LDA 模型</div></div></a></div><div class="next-post pull-right"><a href="/2024/10/08/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC11%E7%AB%A0%20%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/" title="统计学习方法--第11章 条件随机场"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/28/K8FcuStCQYsTV4A.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">统计学习方法--第11章 条件随机场</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/08/29/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95--%E7%AC%AC2%E7%AB%A0%20%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="统计学习方法--第2章 感知机"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/28/K8FcuStCQYsTV4A.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-29</div><div class="title">统计学习方法--第2章 感知机</div></div></a></div><div><a href="/2024/08/30/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95--%E7%AC%AC4%E7%AB%A0%20%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" title="统计学习方法--第4章 朴素贝叶斯"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/28/6xBSQotbm7N13Kn.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-30</div><div class="title">统计学习方法--第4章 朴素贝叶斯</div></div></a></div><div><a href="/2024/08/30/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95--%E7%AC%AC3%E7%AB%A0%20k%E8%BF%91%E9%82%BB%E6%B3%95/" title="统计学习方法--第3章 k近邻法"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/28/LEAbKcnumgeXP8V.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-30</div><div class="title">统计学习方法--第3章 k近邻法</div></div></a></div><div><a href="/2024/09/12/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95--%E7%AC%AC5%E7%AB%A0%20%E5%86%B3%E7%AD%96%E6%A0%91/" title="统计学习方法--第5章 决策树"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/28/ZHLKYkGh4gcCfu6.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-12</div><div class="title">统计学习方法--第5章 决策树</div></div></a></div><div><a href="/2024/09/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC6%E7%AB%A0%20%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%92%8C%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/" title="统计学习方法--第6章 逻辑回归和最大熵模型"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/28/7ISX9YV1U4Fbp3n.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-15</div><div class="title">统计学习方法--第6章 逻辑回归和最大熵模型</div></div></a></div><div><a href="/2024/08/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95--%E7%AC%AC1%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/" title="统计学习方法--第1章 统计学习及监督学习概论"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/28/uDX9laHIOJK6oMv.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-27</div><div class="title">统计学习方法--第1章 统计学习及监督学习概论</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2024/08/28/WOMi84ksFx3dGY1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">LiuJT</div><div class="author-info__description">统计学习笔记&论文阅读</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/TJLJT0929"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/TJLJT0929" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:1007582793@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎大家来到我的博客！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%8D%95%E8%AF%8D%E3%80%81%E8%AF%9D%E9%A2%98%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4"><span class="toc-text">1. 单词、话题向量空间</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E5%8D%95%E8%AF%8D%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4"><span class="toc-text">1.1 单词向量空间</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E8%AF%9D%E9%A2%98%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4"><span class="toc-text">1.2 话题向量空间</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E7%AE%97%E6%B3%95"><span class="toc-text">2. 潜在语义分析算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E7%AE%97%E6%B3%95"><span class="toc-text">3. 非负矩阵分解算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="toc-text">3.1 非负矩阵分解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2 潜在语义分析模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E9%9D%9E%E8%B4%9F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8C%96"><span class="toc-text">3.3 非负矩阵分解的形式化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-%E7%AE%97%E6%B3%95"><span class="toc-text">3.4 算法</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By LiuJT</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="30" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>