<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>统计学习要素--监督学习概览</title>
      <link href="/2024/08/27/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/"/>
      <url>/2024/08/27/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%A7%88/</url>
      
        <content type="html"><![CDATA[<h2 id="监督学习（Supervised-Learning）"><a href="#监督学习（Supervised-Learning）" class="headerlink" title="监督学习（Supervised Learning）"></a>监督学习（Supervised Learning）</h2><p>&emsp;&emsp;监督学习：利用 <strong>输入变量 X</strong> 去预测 <strong>输出变量 Y</strong> .</p><h3 id="1-主要变量类型：定量、定性、定序"><a href="#1-主要变量类型：定量、定性、定序" class="headerlink" title="1.主要变量类型：定量、定性、定序"></a>1.主要变量类型：定量、定性、定序</h3><p>&emsp;&emsp;<strong>定性变量</strong>：</p><ul><li>$\mathbb{G}= { \text{Virginica},\text{Setosa},\text{Versicolor} }$</li><li>$\mathcal {G}={0,1,…,9}$</li></ul><p>输出类型的差别导致对预测的命名规定：预测定量的输出被称为 <strong>回归 ( regression )</strong>，预测定性的输出被称为 <strong>分类 ( classification )</strong> .<br>&emsp;&emsp;<strong>定序变量</strong>：如大、中、小。这些值之间存在顺序，但没有合适的度量概念（不必两两度量之间的差异）。</p><h3 id="2-两种简单预测方式：最小二乘法、最近邻"><a href="#2-两种简单预测方式：最小二乘法、最近邻" class="headerlink" title="2.两种简单预测方式：最小二乘法、最近邻"></a>2.两种简单预测方式：最小二乘法、最近邻</h3><p>&emsp;&emsp;线性模型对结构做出很强的假设（高斯-马尔科夫条件：随机误差项具有零均值、等方差），而且得出稳定但可能不正确的预测。<br>&emsp;&emsp;最近邻方法对结构的假设很宽松，它的预测通常是准确但不稳定的。</p><h4 id="2-1-线性模型："><a href="#2-1-线性模型：" class="headerlink" title="2.1 线性模型："></a>2.1 线性模型：</h4><p>&emsp;&emsp;给定输入变量 $X^T=(X_1,X_2,\cdots,X_p)$，通过</p><script type="math/tex; mode=display">\hat{Y} = \hat{\beta}_0+\sum\limits_{j=1}^{p}X_j\hat{\beta}_j  \tag{1}</script><p>来预测输出 Y ,写成矩阵向量形式：$\hat{Y} = X^T\hat{\beta}$.</p><h4 id="2-2-最小二乘法："><a href="#2-2-最小二乘法：" class="headerlink" title="2.2 最小二乘法："></a>2.2 最小二乘法：</h4><p>通过选取系数 $\beta$ 使得残差平方和最小：</p><script type="math/tex; mode=display">\mathrm{RSS}(\beta) = \sum_{i=1}^N(y_i-x_i^T\beta)^2\tag{2}</script><p>$\mathrm{RSS}(\beta)$是系数的二次函数，因此其最小值总是存在，但可能不唯一。用矩阵向量表示：</p><script type="math/tex; mode=display">\mathrm{RSS}(\beta) = (y-\mathbf{X}\beta)^T(y-\mathbf{X}\beta)\tag{3}</script><p>其中 $\mathbf{X}$ 是 $N\times p$ 矩阵，每一行是一个输入向量，$\mathbf{y}$是训练集里面的 $N$ 维输出向量。对$\beta$求导后，有</p><script type="math/tex; mode=display">\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta)=0\tag{4}</script><p>若 $\mathbf{X}^T\mathbf{X}$ 非奇异（行列式不为零，即矩阵可逆），则有唯一解：</p><script type="math/tex; mode=display">\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\tag{5}</script><h4 id="2-3-最近邻："><a href="#2-3-最近邻：" class="headerlink" title="2.3 最近邻："></a>2.3 最近邻：</h4><p>&emsp;&emsp;最近邻方法：新样本的类别由与新样本距离最近的 $k$ 个训练样本点按照分类决策规则决定。基本做法的三个要点：</p><ul><li>确定距离度量（一般是欧氏距离）</li><li>找出训练集中与带估计点最靠近的 $k$ 个实例点</li><li>分类决策规则：<ul><li>分类任务：投票法，选择这 $k$ 个实例中出现最多的标记类别作为预测结果；</li><li>回归任务：平均法，将这 $k$ 个实例的实值输出标记的平均值作为预测结果;</li></ul></li></ul><p>K近邻最简单实现方法是线性扫描，即计算输入数据与每一个训练数据的距离，当训练集很大时，计算十分耗时。改进：当数据量大时，将数据以树结构呈现。</p><h4 id="2-4-讨论："><a href="#2-4-讨论：" class="headerlink" title="2.4 讨论："></a>2.4 讨论：</h4><p>&emsp;&emsp;最小二乘法通常假设数据是线性可分的，即数据可以用线性决策边界（如直线或超平面）分隔开来。在这种假设下，最小二乘法可以有效地拟合数据使得方差较小，但如果数据本身的分布与线性假设不符，那么模型可能会产生较大的偏差。</p><p>&emsp;&emsp;K-最近邻方法几乎不做关于数据分布的假设。它通过直接比较测试样本与训练样本之间的距离来进行分类，这使得它非常灵活，可以适用于各种复杂的数据分布。因为不需要假设数据具有特定的分布，通常有较低的偏差。另外，对训练数据非常敏感，模型的预测结果可能会受到少量训练数据中噪声的影响，小的训练数据变动或者输入点的微小变化都可能导致判别边界发生显著的变化，从而使得模型的稳定性较差，方差较高。</p><h3 id="3-统计决策理论（构建模型的框架）"><a href="#3-统计决策理论（构建模型的框架）" class="headerlink" title="3.统计决策理论（构建模型的框架）"></a>3.统计决策理论（构建模型的框架）</h3><h4 id="3-1-定量输出情形"><a href="#3-1-定量输出情形" class="headerlink" title="3.1 定量输出情形"></a>3.1 定量输出情形</h4><p>&emsp;&emsp;给定输入 $X$，需要寻找函数 $f(X)$ 来预测 $Y$，需要一个损失函数来度量预测中的错误，一般使用平方误差损失：$L(Y,f(X))=(Y-f(X))^2$，那么就得到了一个关于 $f$ 好坏的度量，即预测误差的期望（Expected Value of Predictive Error）。</p><script type="math/tex; mode=display">\mathrm{EPE}(f)=E(Y-f(X))^2\qquad\qquad\tag{6}</script><script type="math/tex; mode=display">=\int[y-f(x)]^2\mathrm{Pr}(dx,dy)\tag{7}</script><p>在 $X$ 的条件下，可以把 $\mathrm{EPE}$ 写成 ：</p><script type="math/tex; mode=display">\mathrm{EPE}(f) = \mathbb{E}_X\mathbb{E}_{Y\mid X}([Y-f(X)]^2\mid X)\tag{8}</script><p>故目标就是使 $\mathrm{EPE}$ 逐点最小：</p><script type="math/tex; mode=display">f(x) = \argmin_c\mathbb{E}_{Y\mid X}([Y-c]^2\mid X=x)\tag{9}</script><p>解为条件期望：</p><script type="math/tex; mode=display">f(x) = \mathbb{E}(Y\mid X=x)\tag{10}\,,</script><p>也被称作 <strong>回归 (regression)</strong> 函数。因此 $Y$ 在任意点 $X=x$ 处的最优预测为条件均值，“最优”是在均方误差意义下的。</p><p>&emsp;&emsp;最近邻方法直接利用训练数据完成任务。在每一点 $x$ 处，需要输入 $x_i=x$ 附近的所有 $y_i$ 的均值，令：</p><script type="math/tex; mode=display">\hat{f}(x)=\mathrm{Ave}(y_i\mid x_i\in N_k(x))\tag{11}</script><p>其中 $\text{Ave}$ 表示平均，$N_k(x)$ 是集合 $\mathcal{T}$ 中离 $x$ 最近的 $k$ 个点的邻域。这里有两个近似:</p><ul><li>用样本数据的平均来近似期望值；</li><li>在每一点的条件（期望）松弛为在离该目标点近的某个区域上的条件（期望）。</li></ul><p>对于规模为 $N$ 的大规模训练数据，邻域中的点更可能接近 $x$，而且当 $k$ 越大，平均值会更加稳定。事实上，在联合概率分布 $\mathrm{Pr}(X,Y)$ 温和正则条件下，可以证明当 $N,k \longrightarrow \infty$ 使得 $k/N \longrightarrow 0$ 时，$\hat{f}(x) \longrightarrow \mathbb{E}(Y \mid X = x)$。</p><p>&emsp;&emsp;线性回归怎样适应这个框架？最简单的解释是假设回归函数 $f(x)$ 近似线性</p><script type="math/tex; mode=display">f(x)\approx x^T\beta\tag{12}</script><p>将 $f(x)$ 的线性模型插入 $(6)$ 然后微分，可以理论上解出 $\beta$</p><script type="math/tex; mode=display">\beta = [\mathbb{E}(XX^T)]^{-1}\mathbb{E}(XY)\tag{13}</script><p>最小二乘的解 $(5)$ 相当于用训练数据的平均值替换掉 $(13)$ 中的期望。</p><p>所以 $k$-最近邻和最小二乘最终都是根据平均值来近似条件期望。但是它们在模型假设上显著不同：</p><ul><li>最小二乘假设 $f(x)$ 是某个整体线性函数的良好近似。</li><li>$k$-最近邻假设 $f(x)$ 是局部常值函数的良好近似。</li></ul><p>尽管后者似乎更容易被接受，但我们需要为这种灵活性付出代价（维数灾难）。</p><h4 id="3-2-分类输出情形"><a href="#3-2-分类输出情形" class="headerlink" title="3.2 分类输出情形"></a>3.2 分类输出情形</h4><p>&emsp;&emsp;当输出为类变量 $G$ 时，我们需要一个不同的损失函数来惩罚预测错误。预测值 $\hat{G}$ 的取值范围为 $\mathcal{G}$ 。损失函数可写成 $K\times K$ 的矩阵 $\mathbf{L}$ ， $K=\mathrm{card}(\mathbb {G})$。矩阵 $\mathbf{L}$ 对角元素为 $0$ ，其它元素非负。 $L(k,\ell)$ 为把属于 $\mathcal{G}<em>k$ 的类分到 $\mathcal{G}</em>\ell$ 的代价，常用 $0$-$1$ 损失函数，错误分类将受到一个单位的惩罚。关于联合分布 $\mathrm{Pr}(G,X)$ 取期望，预测错误的期望为</p><script type="math/tex; mode=display">\mathrm{EPE} = \mathbb{E}[L(G,\hat{G}(X))]\tag{14}</script><p>再一次考虑条件分布，我们可以写出如下的形式</p><script type="math/tex; mode=display">\mathrm{EPE} = \mathbb{E}_X\sum\limits_{k=1}^KL[{\mathcal{G}}_k,\hat{G}(X)]\mathrm{Pr}({\mathcal{G}}_k\mid X)\tag{15}</script><p>同样地，逐点最小化 $\mathrm{EPE}$ :</p><script type="math/tex; mode=display">\hat{G}(x) = \argmin_{g\in \mathcal{G}}\sum\limits_{k=1}^KL({\mathcal{G}}_k,g)\mathrm{Pr}(\mathcal {G}_k\mid X = x)\tag{16}</script><p>结合 0-1 损失函数上式简化为</p><script type="math/tex; mode=display">\hat{G}(x) = \argmin_{g\in \mathcal{G}}[1 − \mathrm{Pr}(g\mid X = x)]\tag{17}\,</script><script type="math/tex; mode=display"> = \underset{g\in{\mathcal{G}}}{\argmax } \mathrm{Pr}(g\mid X = x)</script><blockquote><p>0-1 损失函数   </p><script type="math/tex; mode=display">L(\mathcal{G}_k, g) = \begin{cases} 0         & \text{if } \ g=\mathcal{G}_k\\        1 & \text{if} \ g\neq \mathcal{G}_k        \end{cases}</script><p>则我们有</p><script type="math/tex; mode=display">\sum_{k=1}^KL({\mathcal{G}}_k,g) \ \mathrm{Pr}(\mathcal {G}_k\mid X = x)</script><script type="math/tex; mode=display">= \sum_{\mathcal{G}_k\neq g}\mathrm{Pr}(\mathcal{G}_k \mid X=x)=1-\mathrm{Pr}(g \mid X=x)</script></blockquote><p>合理的解决方法被称作 <strong>贝叶斯分类 (Bayes classifier)</strong>，利用条件（离散）分布 $\mathrm{Pr}(G\mid X)$ 分到最合适的类别。贝叶斯分类的误差阶被称作 <strong>贝叶斯阶 (Bayes rate)</strong>。</p><h3 id="4-统计模型和函数逼近"><a href="#4-统计模型和函数逼近" class="headerlink" title="4.统计模型和函数逼近"></a>4.统计模型和函数逼近</h3><p>&emsp;&emsp;统计学习的根本目标:寻找一个用输入变量来预测输出变量的隐含关系函数 $f(x)$ 的一个合理的近似。在上一节，从平方误差损失函数出发，我们推导出了量化输出变量的回归方程 $f(x)=\operatorname{E}(Y|X=x)$.最近邻的方法可以直接地估计这个条件期望，但也存在两方面的缺陷：</p><ul><li>如果输入变量空间的维度高，那么最近邻有可能距离目标点不够集中，从而产生较大误差；</li><li>如果对函数存在已知的结构假设，则这个信息可以用来降低估计中的偏差和方差。</li></ul><h4 id="4-1-联合概率分布-operatorname-Pr-X-Y-的统计模型"><a href="#4-1-联合概率分布-operatorname-Pr-X-Y-的统计模型" class="headerlink" title="4.1 联合概率分布$\operatorname{Pr}(X,Y)$的统计模型"></a>4.1 联合概率分布$\operatorname{Pr}(X,Y)$的统计模型</h4><p>&emsp;&emsp;联合概率分布$\operatorname{Pr}(X,Y)$的统计模型：</p><script type="math/tex; mode=display">Y = f(X) + \varepsilon \tag{2.29}</script><p>随机误差项 $\varepsilon$ 满足 $\operatorname{E}(\varepsilon)=0$、与 $X$ 独立，其中 $f(x)=\operatorname{E}(Y|X=x)$</p><h4 id="4-2-函数逼近"><a href="#4-2-函数逼近" class="headerlink" title="4.2 函数逼近"></a>4.2 函数逼近</h4><p>&emsp;&emsp;首先观测输入输出变量空间，获得一个训练样本集 $\mathcal{T}={(x_i,y_i),i=1,\dots,N}$ ，然后将输入变量的值 $x_i$ 输入到一个”学习算法”中，返回一个对应的输出结果 $\hat{f}(x_i)$。算法根据真实的输出变量和算法返回的输出变量的差异 $y_i-\hat{f}(x_i)$ 而自我修正的机制。</p><p>&emsp;&emsp;上述学习范式启发了监督学习问题中的机器学习领域（类比人类的推理过程）和神经网络领域（类比人脑的工作流程）的研究。而从应用数学和统计学出发，是用函数逼近和估计的角度来理解这个过程的。<br>假设观测值${(x_i,y_i)}$，为 $(p+1)$ 维欧式空间中的点，观测值存在这样的模型关系 $y_i=f(x_i)+\varepsilon_i$ 。目标是利用观测样本集合 $\mathcal{T}$ 得到一个合理的 $f(x)$ 的近似值。有效的近似方法可写为线性基函数拓展：</p><script type="math/tex; mode=display">f_\theta(x) = \sum_{k=1}^K h_k(x)\theta_k \tag{2.30}</script><p>其中 $h_k(x)$ 为一组对输入向量 $x$ 的函数，拓展有：</p><ul><li>多项式拓展，$h_k$ 为 $x_1^2$，$x_1x_2^2$</li><li>三角函数拓展，$h_k$ 为 $cos(x_1)$</li><li>非线性扩展，如神经网络模型中的“S 函数”，$h_k(x) = \frac{1}{1 + \exp(-x^T\beta)}$</li></ul><p>对 $f_\theta$ 中的参数 $\theta$，估计方法有：</p><ul><li>最小二乘</li><li>最大似然估计</li></ul><p>在线性模型中，使得残差平方和最小化：$\operatorname{RSS}(\theta) = \sum<em>{i=1}^N (y_i - f</em>\theta(x_i))^2 $</p><h3 id="5-限制性估计的种类"><a href="#5-限制性估计的种类" class="headerlink" title="5.限制性估计的种类"></a>5.限制性估计的种类</h3><h4 id="5-1-粗糙度惩罚"><a href="#5-1-粗糙度惩罚" class="headerlink" title="5.1 粗糙度惩罚"></a>5.1 粗糙度惩罚</h4><p>&emsp;&emsp;这是由显式的惩罚 $\mathrm{RSS}(f)$ 以及粗糙度惩罚控制的函数类别：</p><script type="math/tex; mode=display">\mathrm{PRSS}(f;\lambda)=\mathrm{RSS}(f)+\lambda J(f)\tag{2.38}</script><p>对于在输入空间的小邻域变换太快的函数 $f$，用户选择的函数 $J(f)$ 会变大。举个例子，著名的用于一维输入的 <strong>三次光滑样条 (cubic smoothing spline)</strong> 的是带惩罚的最小二乘的准则的解。</p><script type="math/tex; mode=display">\mathrm{PRSS}(f;\lambda)=\sum\limits_{i=1}^N(y_i-f(x_i))^2+\lambda \int [f''(x)]^2dx\tag{2.39}</script><p>&emsp;&emsp;这里的粗糙惩罚控制了 $f$ 的二阶微分较大的值，而且惩罚的程度由 $\lambda \ge 0$ 来决定。$\lambda=0$ 表示没有惩罚，则可以使用任意插值函数，而 $\lambda=\infty$ 仅仅允许关于 $x$ 的线性函数。</p><h4 id="5-2-核方法"><a href="#5-2-核方法" class="headerlink" title="5.2 核方法"></a>5.2 核方法</h4><p>&emsp;&emsp;<strong>核方法</strong> 是一类把低维空间的非线性可分问题，转化为高维空间的线性可分问题的方法。理论基础：<strong>Cover’s theorem</strong>，对于非线性可分的训练集，可以大概率通过将其非线性映射到一个高维空间来转化成线性可分的训练集。<br>&emsp;&emsp;该方法通过确定局部邻域的本质来显式给出回归函数的估计或条件期望，并且属于局部拟合得很好的规则函数类。局部邻域由 <strong>核函数(kernel function)</strong> $K_{\lambda}(x_0,x)$ 确定，它对 $x_0$ 邻域内的 $x$ 赋予系数，举个例子，高斯核：</p><script type="math/tex; mode=display">K_{\lambda}(x_0,x)=\frac{1}{\lambda}\exp\Big[-\frac{\mid \mid x-x_0\mid \mid ^2}{2\lambda}\Big]\tag{2.40}</script><p>并且对某点赋予与 $x_0$ 的欧氏距离的平方呈指数衰减的权重。系数 $\lambda$ 对应高斯密度的方差，并且控制着邻域的宽度。核估计的最简单形式是 Nadaraya-Watson 的系数平均</p><script type="math/tex; mode=display">\hat{f}(x_0)=\dfrac{\sum_{i=1}^{N}K_{\lambda}(x_0,x_i)y_i}{\sum_{i=1}^NK_{\lambda}(x_0,x_i)}\tag{2.41}</script><p>一般地，我们可以定义 $f(x<em>0)$ 的局部回归估计为 $f</em>{\hat{\theta}}(x_0)$，其中 $\hat{\theta}$ 使下式最小化</p><script type="math/tex; mode=display">\mathrm{RSS}(f_{\theta},x_0)=\sum\limits_{i=1}^NK_{\lambda}(x_0,x_i)(y_i-f_{\theta}(x_i))^2\tag{2.42}</script><p>并且 $f_{\theta}$ 为含参函数，比如低阶的多项式。有如下例子：</p><ul><li>常值函数 $f_{\theta}(x)=\theta_0$，这导出了上式 $(2.41)$ 中的 Nadaraya-Watson 估计。</li><li>$f_{\theta}(x)=\theta_0+\theta_1x$ 给出最受欢迎的局部线性回归模型</li></ul><p>最近邻方法可以看成是某个更加依赖数据的度量的核方法。确实，$k$-最近邻的度量为</p><script type="math/tex; mode=display">K_k(x,x_0)=I(\mid \mid x-x_0\mid \mid \le \mid \mid x_{(k)}-x_0\mid \mid )</script><p>其中 $x_{(k)}$ 是训练观测值中离 $x_0$ 的距离排名第 $k$ 个的观测，而且 $I(S)$ 是集合 $S$ 的指标函数。</p><h4 id="5-3-基函数"><a href="#5-3-基函数" class="headerlink" title="5.3 基函数"></a>5.3 基函数</h4><p>&emsp;&emsp;包括熟悉的线性和多项式展开式，但是最重要的有多种多样的更灵活的模型。这些关于 $f$ 的模型是基本函数的线性展开</p><script type="math/tex; mode=display">f_{\theta}(x)  = \sum\limits_{m=1}^M\theta_mh_m(x)\tag{2.43}</script><p>其中每个 $h_m$ 是 $x$ 的函数，并且其中的线性项与参数 $\theta$ 的行为有关。这个类别包括很多不同的方法。<strong>径向基函数 (Radial basis functions)</strong> 是在质心处对称的 $p$ 维核，</p><script type="math/tex; mode=display">f_{\theta}(x)=\sum\limits_{m=1}^MK_{\lambda_m}(\mu_m,x)\theta_m\tag{2.44}</script><p>举个例子，很流行的高斯核 $K_{\lambda}(\mu,x)=e^{-\mid \mid x-\mu\mid \mid ^2/2\lambda}$。径向基函数的参数有质心 $\mu_m$ 和尺寸 $\lambda_m$，必须要确定这两个值。</p><h3 id="6-偏差-方差的权衡"><a href="#6-偏差-方差的权衡" class="headerlink" title="6.偏差-方差的权衡"></a>6.偏差-方差的权衡</h3><p>&emsp;&emsp;假设数据来自模型 $Y=f(X)+\epsilon, \mathbb{E}(\epsilon)=0,\mathrm{Var}(\epsilon)=\sigma^2$。为了简化，我们假设样本中的值 $x_i$ 提前固定好（不是随机）。在 $x_0$ 处的期望预测误差，也被称为 <strong>测试 (test)</strong> 或 <strong>泛化 (generalization)</strong> 误差，可按如下方式分解：</p><script type="math/tex; mode=display">\begin{align*}\mathrm{EPE}_k(x_0)&=\mathbb{E}[(Y-\hat{f}_k(x_0))^2\mid X=x_0]\\&=\sigma^2+[Bias^2(\hat{f}_k(x_0))+Var_{\mathcal T}(\hat{f}_k(x_0))]\\&=\sigma^2+[f(x_0)-\frac{1}{k}\sum\limits_{\ell=1}^kf(x_{(\ell)})]^2+\frac{\sigma^2}{k}\end{align*}</script><p>带括号的下标 $(\ell)$ 表示 $x_0$ 的最近邻的顺序。在展开式中有三项。</p><ul><li><p>第一项 $\sigma^2$ 是 <strong>不可约减的 (irreducible)</strong> 误差——是新测试目标点的方差——而且我们不能够控制，即使我们知道真值 $f(x_0)$</p></li><li><p>第二项和第三项在我们的控制范围内，并且构成了估计 $f(x<em>0)$ 时 $\hat f_k(x_0)$ 的 <strong>均方误差 (mean squared error)</strong>，均方误差经常被分解成偏差部分和方差部分。偏差项是真值均值 $f(x_0)$ 与估计的期望值之间差异的平方——$[\mathbb{E}</em>{\mathcal T}(\hat{f}_k(x<em>0))-f(x_0)]^2$——其中期望平均了训练数据中的随机量。如果真实的函数相当地光滑，这一项很可能随着 $k$ 的增加而增加。对于较小的 $k$ 值和较少的近邻点会导致值 $f(x</em>{(\ell)})$ 与 $f(x_0)$ 很接近，所以它们的平均应该距离 $f(x_0)$ 很近。当 $k$ 值增加，邻域远离，然后任何事情都可能发生。</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/08/27/hello-world/"/>
      <url>/2024/08/27/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
